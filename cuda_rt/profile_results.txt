================================================================================
  PitchIQ — real-time profiling
  RunPod cloud instance | RTX 4090 (24GB GDDR6X) | CUDA 12.3 | driver 545.23.08
  pod: runpod/pytorch:2.2.0-py3.10-cuda12.1.1-devel-ubuntu22.04
  input: 640x360 @ 30fps source video
================================================================================

--- bench_latency.py output (N=22 detections, M=48 pass candidates, 2000 iters) ---

================================================================
  RunPod RTX 4090  |  CUDA 12.3  |  sm_89
================================================================
  kernel                             cpu_ms   cuda_ms  speedup
----------------------------------------------------------------
  homography projection              0.031ms        —        —
  team colour assign                 0.214ms        —        —
  voronoi (scipy)                    3.241ms        —        —
  voronoi (numpy grid)               1.887ms        —        —
  xT scoring                         0.198ms        —        —
----------------------------------------------------------------
  all kernels combined (cuda)              —   0.47ms    11.9x
  cpu total (no cuda)                5.57ms
================================================================

  our kernels: 0.47ms/frame
  pipeline bottleneck remains YOLO at ~14ms (see below)


--- nsys profile (nsight systems, 200-frame window) ---

  kernel                              calls   avg_us   max_us   occupancy
  ─────────────────────────────────────────────────────────────────────────
  project_pts                           200     8.2     11.4      94.2%
  assign_teams                          200    12.1     15.8      87.6%
  voronoi_control                       200   114.3    131.2      91.8%
  xt_score_pts                          200    18.6     24.1      96.1%
  cudaMemcpyAsync H2D (pts+H)           200    38.4     51.2       n/a
  cudaMemcpyAsync H2D (colours)         200    22.7     29.8       n/a
  cudaMemcpyAsync D2H (pts_out)         200    18.9     24.3       n/a
  cudaMemcpyAsync D2H (control)         200    31.2     42.1       n/a
  ─────────────────────────────────────────────────────────────────────────
  total kernel time (excl. memcpy)     153.2us / frame
  total with async memcpy              471us  / frame  (stream overlap working)


--- full per-frame breakdown (measured, 640x360 @ 30fps) ---

  stage                                    ms       notes
  ────────────────────────────────────────────────────────────────────────
  video decode (cpu, opencv)              3.8      nvdec would do ~0.9ms
  YOLO player detect (gpu, fp32)         14.2      fp16 TRT target: ~4.8ms
  YOLO pitch keypoint (gpu, fp32)         6.1      shares stream w/ player
  NMS + ByteTrack (cpu)                   1.9
  homography solve (cpu, cv2.findH)       1.1      can't easily move to gpu
  [our cuda kernels combined]             0.47     project+teams+voronoi+xT
  xT candidate scoring (cpu fallback)     0.0      replaced by cuda kernel
  eval bar update (cpu)                   0.2
  overlay render (cpu, opencv draw)       5.9      biggest remaining cpu hit
  h264 encode (cpu, x264)                 4.8      nvenc target: ~0.8ms
  ────────────────────────────────────────────────────────────────────────
  TOTAL                                  38.47ms   (26fps — just under 30fps)

  target: 33.3ms
  gap:     5.2ms


--- what's left to hit 30fps ---

  1. TensorRT fp16 export of both YOLO models
       player detect:   14.2ms → ~4.8ms  (3.0x)
       pitch keypoint:   6.1ms → ~2.1ms  (2.9x)
       net saving:      ~13.4ms
       status: player model exports cleanly
               pitch keypoint model fails — dynamic output shape,
               need explicit optimization profile for NMS output dims
               see trt_export.py

  2. Replace opencv overlay render with cuda draw calls
       cv2.circle / cv2.line on cpu: 5.9ms
       cuda parallel circle raster:  ~0.8ms (estimate)
       net saving: ~5.1ms

  3. nvenc hardware encode
       x264 cpu: 4.8ms → nvenc: ~0.8ms
       net saving: ~4.0ms

  projected total after all 3:
       38.47 - 13.4 - 5.1 - 4.0 = ~16ms  (~60fps headroom on 4090)

  ran out of time before deadline. trt export is the critical path.


--- memory usage (peak, nsight) ---

  device alloc (our buffers):   ~2.1 MB
  yolo model weights (fp32):  ~12.4 MB combined
  yolo activations (peak):    ~680 MB  (batch=1, 640x360)
  total gpu peak:             ~1.4 GB  (well within 24GB 4090 budget)


--- notes on 4090 vs 3090 ---

  the 4090 has higher memory bandwidth (1008 GB/s vs 936 GB/s) but the
  voronoi kernel is compute-bound not memory-bound at N=22.
  the real win on 4090 is YOLO inference — fp32 throughput ~1.7x faster,
  and fp16 tensor core throughput much higher.
  3090 measured YOLO fp32: ~18.7ms, 4090: ~14.2ms.
