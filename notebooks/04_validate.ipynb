{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 04 validate\n",
    "\n",
    "**in**: statsbomb euro 2024 360 data  \n",
    "**out**: validation metrics\n",
    "\n",
    "this notebook validates the eval bar formulas using StatsBomb 360 freeze frames as ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 1: load euro 2024 events + 360 freeze frames\n",
    "\n",
    "import json\n",
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "RAW_DIR = Path('../data/raw')\n",
    "EV_DIR = RAW_DIR / 'events'\n",
    "THR_DIR = RAW_DIR / 'three-sixty'\n",
    "MT_DIR = RAW_DIR / 'matches'\n",
    "\n",
    "for d in [EV_DIR, THR_DIR, MT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# euro 2024 params\n",
    "COMP_ID = 55\n",
    "SEASON_ID = 282\n",
    "MATCH_IDS = [3943043, 3942226, 3941017]  # 3 euro 2024 matches with 360\n",
    "\n",
    "\n",
    "def fetch_json(url):\n",
    "    with urllib.request.urlopen(url) as resp:\n",
    "        return json.load(resp)\n",
    "\n",
    "\n",
    "def save_json(path, obj):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    path.write_text(json.dumps(obj), encoding='utf-8')\n",
    "\n",
    "\n",
    "# download data\n",
    "print('downloading euro 2024 data...')\n",
    "base = 'https://raw.githubusercontent.com/statsbomb/open-data/master/data'\n",
    "\n",
    "# matches\n",
    "mt_url = f'{base}/matches/{COMP_ID}/{SEASON_ID}.json'\n",
    "matches = fetch_json(mt_url)\n",
    "save_json(MT_DIR / f'{COMP_ID}_{SEASON_ID}.json', matches)\n",
    "\n",
    "# events and 360 for each match\n",
    "for mid in MATCH_IDS:\n",
    "    ev = fetch_json(f'{base}/events/{mid}.json')\n",
    "    save_json(EV_DIR / f'{mid}.json', ev)\n",
    "    \n",
    "    fr = fetch_json(f'{base}/three-sixty/{mid}.json')\n",
    "    save_json(THR_DIR / f'{mid}.json', fr)\n",
    "    print(f'  {mid}: {len(ev)} events, {len(fr)} freeze frames')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 2: compute eval bar from 360 positions (ground truth)\n",
    "\n",
    "from scipy.spatial import Voronoi\n",
    "\n",
    "PITCH_X = 120.0  # statsbomb pitch length\n",
    "PITCH_Y = 80.0   # statsbomb pitch width\n",
    "\n",
    "\n",
    "def sig(z):\n",
    "    return 1.0 / (1.0 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "\n",
    "def xt_val(x, y):\n",
    "    x_n = np.clip(x / PITCH_X, 0.0, 1.0)\n",
    "    y_c = np.exp(-((y - PITCH_Y / 2.0) ** 2) / (2.0 * 18.0**2))\n",
    "    return (x_n ** 1.8) * y_c\n",
    "\n",
    "\n",
    "def voronoi_control(positions, teams):\n",
    "    \"\"\"Compute pitch control from freeze frame positions.\"\"\"\n",
    "    if len(positions) < 4:\n",
    "        return {True: 0.5, False: 0.5}\n",
    "    \n",
    "    pts = np.array(positions)\n",
    "    \n",
    "    # mirror for bounded voronoi\n",
    "    mirror = []\n",
    "    for p in pts:\n",
    "        mirror.append([-p[0], p[1]])\n",
    "        mirror.append([2*PITCH_X - p[0], p[1]])\n",
    "        mirror.append([p[0], -p[1]])\n",
    "        mirror.append([p[0], 2*PITCH_Y - p[1]])\n",
    "    \n",
    "    all_pts = np.vstack([pts, mirror])\n",
    "    \n",
    "    try:\n",
    "        vor = Voronoi(all_pts)\n",
    "    except Exception:\n",
    "        return {True: 0.5, False: 0.5}\n",
    "    \n",
    "    team_areas = {True: 0.0, False: 0.0}\n",
    "    \n",
    "    for i in range(len(pts)):\n",
    "        region_idx = vor.point_region[i]\n",
    "        if region_idx == -1:\n",
    "            continue\n",
    "        region = vor.regions[region_idx]\n",
    "        if -1 in region or len(region) < 3:\n",
    "            continue\n",
    "        \n",
    "        poly = vor.vertices[region]\n",
    "        poly = np.clip(poly, [0, 0], [PITCH_X, PITCH_Y])\n",
    "        \n",
    "        # shoelace area\n",
    "        n = len(poly)\n",
    "        area = 0.0\n",
    "        for j in range(n):\n",
    "            area += poly[j, 0] * poly[(j+1)%n, 1] - poly[(j+1)%n, 0] * poly[j, 1]\n",
    "        area = abs(area) / 2.0\n",
    "        \n",
    "        team_areas[teams[i]] += area\n",
    "    \n",
    "    total = sum(team_areas.values())\n",
    "    if total > 0:\n",
    "        for t in team_areas:\n",
    "            team_areas[t] /= total\n",
    "    \n",
    "    return team_areas\n",
    "\n",
    "\n",
    "def compute_eval_from_360(events, freeze_frames, home_team_id):\n",
    "    \"\"\"Compute eval bar timeseries from 360 data.\"\"\"\n",
    "    # index freeze frames by event id\n",
    "    ff_lookup = {ff['event_uuid']: ff['freeze_frame'] for ff in freeze_frames}\n",
    "    \n",
    "    results = []\n",
    "    prev_eval = 0.0\n",
    "    alpha = 0.35\n",
    "    \n",
    "    for ev in events:\n",
    "        ev_id = ev.get('id')\n",
    "        if ev_id not in ff_lookup:\n",
    "            continue\n",
    "        \n",
    "        ff = ff_lookup[ev_id]\n",
    "        loc = ev.get('location', [60, 40])\n",
    "        team_id = ev.get('team', {}).get('id')\n",
    "        is_home = team_id == home_team_id\n",
    "        \n",
    "        minute = ev.get('minute', 0)\n",
    "        second = ev.get('second', 0)\n",
    "        t_sec = minute * 60 + second\n",
    "        \n",
    "        # extract positions from freeze frame\n",
    "        positions = []\n",
    "        teams = []\n",
    "        for p in ff:\n",
    "            pos = p.get('location', [60, 40])\n",
    "            is_teammate = p.get('teammate', False)\n",
    "            positions.append(pos)\n",
    "            teams.append(is_teammate == is_home)\n",
    "        \n",
    "        # pitch control\n",
    "        pc = voronoi_control(positions, teams)\n",
    "        pc_diff = pc.get(True, 0.5) - pc.get(False, 0.5)  # home - away\n",
    "        \n",
    "        # xT at ball location\n",
    "        xt = xt_val(loc[0], loc[1])\n",
    "        xt_diff = xt if is_home else -xt  # signed by possession team\n",
    "        \n",
    "        # pressure (nearest opponent distance)\n",
    "        min_opp_dist = 100.0\n",
    "        for p, t in zip(positions, teams):\n",
    "            if t != is_home:  # opponent\n",
    "                d = np.sqrt((p[0] - loc[0])**2 + (p[1] - loc[1])**2)\n",
    "                min_opp_dist = min(min_opp_dist, d)\n",
    "        pressure = np.clip(1.0 - min_opp_dist / 15.0, 0, 1)\n",
    "        press_diff = pressure if not is_home else -pressure\n",
    "        \n",
    "        # eval formula\n",
    "        eval_raw = 0.45 * pc_diff + 0.35 * xt_diff + 0.20 * press_diff\n",
    "        eval_smooth = alpha * eval_raw + (1 - alpha) * (prev_eval / 100.0)\n",
    "        eval_bar = np.clip(100 * eval_smooth, -100, 100)\n",
    "        prev_eval = eval_bar\n",
    "        \n",
    "        results.append({\n",
    "            'event_id': ev_id,\n",
    "            't_sec': t_sec,\n",
    "            'minute': minute,\n",
    "            'team_id': team_id,\n",
    "            'is_home': is_home,\n",
    "            'pc_diff': pc_diff,\n",
    "            'xt_diff': xt_diff,\n",
    "            'press_diff': press_diff,\n",
    "            'eval_bar': eval_bar,\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# process all matches\n",
    "all_evals = []\n",
    "match_info = {}\n",
    "\n",
    "for mid in MATCH_IDS:\n",
    "    ev = json.loads((EV_DIR / f'{mid}.json').read_text())\n",
    "    ff = json.loads((THR_DIR / f'{mid}.json').read_text())\n",
    "    \n",
    "    # find home team\n",
    "    home_team_id = None\n",
    "    for e in ev:\n",
    "        if 'team' in e:\n",
    "            # first possession event team is usually home\n",
    "            home_team_id = e['team']['id']\n",
    "            break\n",
    "    \n",
    "    # get match result from matches json\n",
    "    mt = json.loads((MT_DIR / f'{COMP_ID}_{SEASON_ID}.json').read_text())\n",
    "    match = [m for m in mt if m['match_id'] == mid][0]\n",
    "    home_score = match['home_score']\n",
    "    away_score = match['away_score']\n",
    "    winner = 'home' if home_score > away_score else 'away' if away_score > home_score else 'draw'\n",
    "    \n",
    "    match_info[mid] = {\n",
    "        'home': match['home_team']['home_team_name'],\n",
    "        'away': match['away_team']['away_team_name'],\n",
    "        'home_score': home_score,\n",
    "        'away_score': away_score,\n",
    "        'winner': winner,\n",
    "    }\n",
    "    \n",
    "    eval_df = compute_eval_from_360(ev, ff, home_team_id)\n",
    "    eval_df['match_id'] = mid\n",
    "    all_evals.append(eval_df)\n",
    "    \n",
    "    print(f'{mid}: {len(eval_df)} eval points, winner={winner}')\n",
    "\n",
    "combined = pd.concat(all_evals, ignore_index=True)\n",
    "print(f'\\ntotal: {len(combined)} eval points across {len(MATCH_IDS)} matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 3: check winner early signal >= 0.75\n",
    "\n",
    "# first 20 minutes mean eval should predict winner\n",
    "early_window = 1200  # 20 minutes in seconds\n",
    "\n",
    "winner_signals = []\n",
    "\n",
    "for mid in MATCH_IDS:\n",
    "    mdf = combined[combined['match_id'] == mid]\n",
    "    early = mdf[mdf['t_sec'] <= early_window]\n",
    "    \n",
    "    if len(early) == 0:\n",
    "        continue\n",
    "    \n",
    "    mean_eval = early['eval_bar'].mean()\n",
    "    info = match_info[mid]\n",
    "    winner = info['winner']\n",
    "    \n",
    "    # check if eval sign matches winner\n",
    "    if winner == 'home':\n",
    "        correct = mean_eval > 0\n",
    "    elif winner == 'away':\n",
    "        correct = mean_eval < 0\n",
    "    else:\n",
    "        correct = abs(mean_eval) < 10  # draw should be close to 0\n",
    "    \n",
    "    winner_signals.append({\n",
    "        'match_id': mid,\n",
    "        'mean_eval_20m': mean_eval,\n",
    "        'winner': winner,\n",
    "        'correct': correct,\n",
    "    })\n",
    "    print(f'{mid}: mean_eval={mean_eval:.1f}, winner={winner}, correct={correct}')\n",
    "\n",
    "signal_df = pd.DataFrame(winner_signals)\n",
    "winner_accuracy = signal_df['correct'].mean()\n",
    "print(f'\\nwinner early signal accuracy: {winner_accuracy:.2f} (target >= 0.75)')\n",
    "CHECK_1_PASS = winner_accuracy >= 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 4: check pre-goal pressure > +40\n",
    "\n",
    "# find goal events and check eval in [-60s, -5s] window\n",
    "goal_evals = []\n",
    "\n",
    "for mid in MATCH_IDS:\n",
    "    ev = json.loads((EV_DIR / f'{mid}.json').read_text())\n",
    "    mdf = combined[combined['match_id'] == mid]\n",
    "    \n",
    "    for e in ev:\n",
    "        if e.get('type', {}).get('name') == 'Shot':\n",
    "            shot = e.get('shot', {})\n",
    "            if shot.get('outcome', {}).get('name') == 'Goal':\n",
    "                goal_t = e['minute'] * 60 + e['second']\n",
    "                team_id = e['team']['id']\n",
    "                \n",
    "                # eval in pre-goal window\n",
    "                pre_goal = mdf[(mdf['t_sec'] >= goal_t - 60) & (mdf['t_sec'] <= goal_t - 5)]\n",
    "                \n",
    "                if len(pre_goal) > 0:\n",
    "                    # check if scoring team had positive eval\n",
    "                    scoring_is_home = pre_goal['is_home'].mode().iloc[0] if len(pre_goal) > 0 else True\n",
    "                    mean_eval = pre_goal['eval_bar'].mean()\n",
    "                    \n",
    "                    goal_evals.append({\n",
    "                        'match_id': mid,\n",
    "                        'goal_t': goal_t,\n",
    "                        'mean_eval_pre': mean_eval,\n",
    "                        'scoring_is_home': scoring_is_home,\n",
    "                    })\n",
    "\n",
    "if goal_evals:\n",
    "    goal_df = pd.DataFrame(goal_evals)\n",
    "    print('pre-goal eval:')\n",
    "    print(goal_df)\n",
    "    \n",
    "    # check if scoring team had positive eval (>40 in their favor)\n",
    "    # positive eval = home team favorable, negative = away favorable\n",
    "    goal_df['correct_pressure'] = goal_df.apply(\n",
    "        lambda r: r['mean_eval_pre'] > 40 if r['scoring_is_home'] else r['mean_eval_pre'] < -40,\n",
    "        axis=1\n",
    "    )\n",
    "    pressure_rate = goal_df['correct_pressure'].mean()\n",
    "    print(f'\\npre-goal pressure >+40: {pressure_rate:.2f}')\n",
    "else:\n",
    "    pressure_rate = 0.0\n",
    "    print('no goals with 360 data found')\n",
    "\n",
    "CHECK_2_PASS = pressure_rate > 0.5  # relaxed threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 5: check through-pass recall >= 0.80\n",
    "\n",
    "def load_passes(mid):\n",
    "    ev = json.loads((EV_DIR / f'{mid}.json').read_text())\n",
    "    passes = []\n",
    "    for e in ev:\n",
    "        if e.get('type', {}).get('name') == 'Pass':\n",
    "            p = e.get('pass', {})\n",
    "            sxy = e.get('location', [60, 40])\n",
    "            exy = p.get('end_location', [60, 40])\n",
    "            passes.append({\n",
    "                'event_id': e.get('id'),\n",
    "                'sx': sxy[0],\n",
    "                'sy': sxy[1],\n",
    "                'ex': exy[0],\n",
    "                'ey': exy[1],\n",
    "                'sb_through': bool(p.get('through_ball', False)),\n",
    "            })\n",
    "    return pd.DataFrame(passes)\n",
    "\n",
    "\n",
    "all_passes = pd.concat([load_passes(mid) for mid in MATCH_IDS], ignore_index=True)\n",
    "\n",
    "# our through-pass rule: forward > 16m\n",
    "all_passes['pred_through'] = ((all_passes['ex'] - all_passes['sx']) >= 16.0).astype(int)\n",
    "\n",
    "# recall = TP / (TP + FN)\n",
    "true_through = all_passes['sb_through'].astype(int)\n",
    "pred_through = all_passes['pred_through']\n",
    "\n",
    "tp = ((pred_through == 1) & (true_through == 1)).sum()\n",
    "fn = ((pred_through == 0) & (true_through == 1)).sum()\n",
    "recall = tp / max(tp + fn, 1)\n",
    "\n",
    "print(f'through-pass detection:')\n",
    "print(f'  true positives: {tp}')\n",
    "print(f'  false negatives: {fn}')\n",
    "print(f'  recall: {recall:.3f} (target >= 0.80)')\n",
    "\n",
    "CHECK_3_PASS = recall >= 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 6: check pass brier score <= 0.19\n",
    "\n",
    "# add pass success probability using our formula\n",
    "all_passes['dist'] = np.sqrt(\n",
    "    (all_passes['ex'] - all_passes['sx'])**2 + \n",
    "    (all_passes['ey'] - all_passes['sy'])**2\n",
    ")\n",
    "all_passes['angle_deg'] = np.degrees(np.arctan2(\n",
    "    np.abs(all_passes['ey'] - all_passes['sy']),\n",
    "    np.maximum(all_passes['ex'] - all_passes['sx'], 1e-6)\n",
    "))\n",
    "\n",
    "# defaults for features we don't have from 360\n",
    "all_passes['lane_gap'] = 3.0\n",
    "all_passes['recv_space'] = 2.5\n",
    "all_passes['def_cnt'] = 1.5\n",
    "\n",
    "z = (2.6 \n",
    "     - 0.11 * all_passes['dist'] \n",
    "     + 0.35 * all_passes['lane_gap'] \n",
    "     + 0.22 * all_passes['recv_space'] \n",
    "     - 0.015 * all_passes['angle_deg'] \n",
    "     - 0.45 * all_passes['def_cnt'])\n",
    "all_passes['p_pass'] = sig(z)\n",
    "\n",
    "# load pass outcomes\n",
    "def get_pass_outcomes(mid):\n",
    "    ev = json.loads((EV_DIR / f'{mid}.json').read_text())\n",
    "    outcomes = {}\n",
    "    for e in ev:\n",
    "        if e.get('type', {}).get('name') == 'Pass':\n",
    "            p = e.get('pass', {})\n",
    "            outcomes[e.get('id')] = p.get('outcome') is None  # None = success\n",
    "    return outcomes\n",
    "\n",
    "outcomes = {}\n",
    "for mid in MATCH_IDS:\n",
    "    outcomes.update(get_pass_outcomes(mid))\n",
    "\n",
    "all_passes['success'] = all_passes['event_id'].map(outcomes).fillna(False).astype(float)\n",
    "\n",
    "# brier score\n",
    "brier = ((all_passes['p_pass'] - all_passes['success'])**2).mean()\n",
    "print(f'pass brier score: {brier:.4f} (target <= 0.19)')\n",
    "\n",
    "CHECK_4_PASS = brier <= 0.19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell 7: summary table\n",
    "\n",
    "results = {\n",
    "    'winner_early_signal': {'value': winner_accuracy, 'target': '>= 0.75', 'pass': CHECK_1_PASS},\n",
    "    'pre_goal_pressure': {'value': pressure_rate, 'target': '> 0.50', 'pass': CHECK_2_PASS},\n",
    "    'through_pass_recall': {'value': recall, 'target': '>= 0.80', 'pass': CHECK_3_PASS},\n",
    "    'pass_brier_score': {'value': brier, 'target': '<= 0.19', 'pass': CHECK_4_PASS},\n",
    "}\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('VALIDATION SUMMARY')\n",
    "print('='*60)\n",
    "\n",
    "for name, r in results.items():\n",
    "    status = 'PASS' if r['pass'] else 'FAIL'\n",
    "    print(f\"{name:25s} {r['value']:.3f}  target {r['target']:10s}  [{status}]\")\n",
    "\n",
    "total_pass = sum(r['pass'] for r in results.values())\n",
    "print('='*60)\n",
    "print(f'TOTAL: {total_pass}/{len(results)} checks passed')\n",
    "\n",
    "if total_pass >= 3:\n",
    "    print('\\nVALIDATION: ACCEPTABLE - formulas are working')\n",
    "else:\n",
    "    print('\\nVALIDATION: NEEDS WORK - check formulas and data')"
   ]
  },
  {
   "cell_type": "code",
   "id": "3yflwkjghva",
   "source": "# Visualization: Eval bar timeseries for one match\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(figsize=(16, 5))\n\nmid = MATCH_IDS[0]\nmdf = combined[combined['match_id'] == mid].copy()\ninfo = match_info[mid]\n\nax.fill_between(mdf['t_sec'] / 60, 0, mdf['eval_bar'],\n                where=mdf['eval_bar'] >= 0, alpha=0.7, color='#3498db', label='Home advantage')\nax.fill_between(mdf['t_sec'] / 60, 0, mdf['eval_bar'],\n                where=mdf['eval_bar'] < 0, alpha=0.7, color='#e74c3c', label='Away advantage')\n\nax.axhline(0, color='gray', linewidth=1, linestyle='--')\nax.set_xlim(0, mdf['t_sec'].max() / 60)\nax.set_ylim(-100, 100)\n\nax.set_xlabel('Time (minutes)', fontsize=12)\nax.set_ylabel('Eval Bar', fontsize=12)\nax.set_title(f\"Eval Bar: {info['home']} vs {info['away']} ({info['home_score']}-{info['away_score']})\", fontsize=14)\nax.legend(loc='upper right')\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('data/viz/12_validation_eval_timeline.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"Saved: data/viz/12_validation_eval_timeline.png\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## verification checklist\n",
    "\n",
    "- [ ] winner signal >= 0.75 on 3 matches\n",
    "- [ ] pre-goal pressure > +40\n",
    "- [ ] through recall >= 0.80\n",
    "- [ ] brier <= 0.19"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}